# streamlit-job-recommender

# 프로젝트 개요
*	동국대학교 2025년도 1차 대학원 프로젝트 지원사업의 지원으로 진행된 프로젝트임
*	해당 프로젝트는 대학원생의 자기주도형 공동연구 경험과 전공 연구분야의 전문성을 고취시키기 위한 대학원생 지원 프로젝트 지원 사업임
*	데이터 분석 직무 채용공고를 대상으로 비정형 텍스트 정보를 구조화하고 직무 특성과 요구 역량을 분석·시각화하는 시스템 구축 프로젝트임

# 프로젝트 목적
*	채용 공고 사이트에 게시된 정보가 서로 다른 표현 방식으로 작성되어 있으며, 업무 내용 및 우대사항이 정량화되어 있지 않아 구직자가 직관적으로 이해하기 어려운 문제 해결을 목표로 함.
*	비정형 채용공고 텍스트를 구직자에게 필요한 형태로 가공·표준화하여, 기업이 요구하는 역량 및 스킬을 체계적으로 파악할 수 있도록 지원하는 목적임
*	데이터 분석 직무 내에서도 세부직무별로 상이한 요구 역량이 존재함에도 이를 객관적으로 비교·분석하기 어려운 한계를 해소하여 구직자가 희망하는 세부직무 유형을 명확히 인지할 수 있도록 함.
*	세부직무별로 요구되는 추천 스킬 조합을 시각적으로 제시하여, 구직자의 역량 강화 방향 설정을 지원하고자 개인 역량과 해당 직무 집단의 평균 역량을 Radar Chart 기반으로 비교 시각화하여, 본인의 강점 및 부족한 역량을 직관적으로 확인할 수 있도록 지원함.

# 구현 방식
1. Pdf 크롤링
   
    1-1. 링크 수집 
      * 검색 결과 페이지에서 공고 링크를 모을 때, 이미 본 링크는 제외
      *	어떤 페이지에서 신규 링크가 0개면 더 이상 페이지를 넘기지 않고 종료
        
    1-2 원치 않는 공고 사전 필터링
      *	취업캠프/부트캠프/교육생 등 목적 외 키워드 포함 시 제외함
        
    1-3 페이지 로딩이 덜 된 상태로 pdf가 저장되는 문제
      *	상세페이지 진입 후, 여러 후보 셀렉터 중 하나라도 “visible”이면 로딩 성공으로 간주
      *	PDF 저장이 실패/타임아웃이거나, PDF 파일 크기가 너무 작으면 실패로 보고 재시도
      *	재시도 시 backoff(대기시간 증가) + about:blank로 리셋 시도

2. OCR 전처리
   
    2-1 OCR 엔진 선택 및 설정
      *	PaddleOCR를 사용하고, 텍스트 라인 방향 분류를 켜서(기울어짐 등) 인식 안정성을 높임
      *	PaddleOCR에서 권장되는 predict()를 쓰되, 결과 포맷이 환경마다 달라질 수 있어서 predict 결과를 기존 [box,(text,score)] 형태로 변환하는 어댑터를 추가함.
        
    2-2 Layout 기반 광고 제거
      *	OCR이 화면의 UI/광고까지 텍스트로 추출하기 때문에 바운딩 박스 좌표를 써서 우측 하단을 통째로 제외하도록 함
        
    2-3 Rule-based 텍스트 정제
      *	OCR 텍스트에서 채용 사이트 공통 푸터/정책/연락처 같은 반복 노이즈를 제거하기 위해 정규식 기반 라인 삭제
      *	취업전략/관련태그처럼 큰 덩어리 블록을 시작-끝 패턴으로 삭제함
        
    2-4 섹션화(회사소개/담당업무/자격요건/우대사항) 로직
      *	정제된 텍스트를 순회하면서 헤더 후보를 찾고, 해당 헤더 이후 라인을 현재 섹션에 누적하는 방식
      *	헤더 표기를 통일하기 위한 매핑 정의
  
3. 추가 전처리
   
    3-1. Rule-based 추출
      *	불포함 단어, 문장 패턴 제거
      *	로그인/회원가입/고객센터/브랜드 스토리 등 불필요 문장 제거
      *	스킬은 매핑 사전 기반으로 탐지
        
    3-2. 세부직무명 LLM 분류
      *	직무/모집분야/포지션과 같은 줄을 앵커(anchor)로 잡고, 그 앵커의 앞뒤 일정 범위(window) 텍스트에서만 후보 라벨을 찾음. 그 안에서 후보 라벨이 매칭되면 그걸 세부직무명으로 정함.
      *	담당업무, 우대사항 기반 매칭 : 담당업무, 우대사항이 포함된 ocr 텍스트에서 후보 라벨 매칭을 시도 -> 섹션 인식이 실패했을 때를 대비해, 전체 텍스트에서도 1회 추가 매칭을 시도함
      *	Pdf 제목 기반 매칭 : pdf 파일명에서도 후보 라벨 매칭 시도
      *	LLM fallback : 후보 리스트를 고정으로 주고, 입력 텍스트 근거로 후보 중 단 하나를 고르게 하는 1-shot 분류
      
    3-3 LLM 프롬프트 
      *	출력은 json 형식으로 하기
      *	추가 설명, 새로운 세부직무명 생성 금지
      *	입력 텍스트 길이 제한, 모델 입력 토큰 상한 설정, 생성 안정화(temperature=0.0, top_p=1.0), 출력에서 json만 파싱(설명 섞일 경우 무시), 
      *	최종 산출물 형태 : 세부직무명, 회사명, 담당업무, 우대사항, 스킬 json 파일

4. LLM 핵심 역량 산출
   
    4-1. 입력 구성 및 캐시 확인
      *	공고별 전처리 결과(담당업무, 우대사항, 스킬)를 결합하여 LLM 입력 텍스트로 구성함
      *	공고 고유 ID(uid)를 기준으로 기존에 산출된 역량 점수가 캐시에 존재하는 경우, LLM 호출 없이 즉시 재사용함
        
    4-2. LLM 기반 핵심 역량 점수 산출
      *	LLM을 활용하여 6대 핵심 역량을 1~5점 정수로 평가하도록 프롬프트를 구성함
      *	각 역량 점수는 입력 텍스트에 포함된 업무 내용과 요구 스킬에 근거하여 점수화하도록 지시함
        
    4-3. 출력 파싱 및 정규화
      *	LLM 출력에서 JSON 객체만 추출하여 파싱함
      *	누락된 역량 항목은 기본값으로 보정하고, 모든 점수를 1~5 범위로 제한(clamp)함
        
    4-4. 결과 저장 및 재사용
      *	최종 역량 점수를 공고 ID 기준으로 캐시 파일(JSON)에 저장함
      *	동일 공고에 대한 반복 호출 시 연산 비용을 줄이기 위해 캐시 기반 재사용 구조를 적용함

5. 시각화
   
    5-1. 임베딩·클러스터·UMAP 전처리
      *	담당업무와 스킬 텍스트를 결합하여 임베딩 입력 텍스트를 생성함 
      *	SBERT 임베딩 생성 후 KMeans로 클러스터를 산출하고, UMAP으로 2차원 좌표(x,y)를 생성함 
      *	시각화에 필요한 컬럼(uid, 회사명, 세부직무명, cluster, x, y, 담당업무, 우대사항, 스킬)을 parquet로 저장함
        
    5-2. 클러스터 특징 기반 라벨 부여
      *	클러스터별 텍스트 blob(담당업무/우대사항/스킬/세부직무명 결합)을 생성하여 클러스터 특성 텍스트를 구성함 
      *	라벨별 키워드 출현 빈도를 계산하고, 클러스터 6역량 평균을 라벨별로 가중하여 라벨 점수를 산출함(예: 엔지니어링은 SQL/DB 가중, BI/시각화는 시각화 가중 등) 
      *	클러스터 × 라벨 점수표를 만든 뒤, 4개 라벨을 클러스터에 중복 없이 배정하는 모든 경우(4! = 24)를 탐색하여 총점이 최대가 되는 매핑을 선택함 
      *	최종 cluster→라벨 매핑을 cluster_labels_final_k4.json으로 저장하고, 필요 시 수동 swap 함수로 교정 가능하도록 구성함 
      * 클러스터 특징을 키워드, 6역량 평균 기반 규칙 점수화, 최적할당으로 이름 붙이는 구조임.
        
    5-3. 세부직무 유사도(UMAP) 시각화(색상/hover)
      *	UMAP 산점도에서 공고 간 유사도를 2차원 좌표로 표현함 
      *	cluster_label을 만들어 세부직무 별로 색상을 다르게 표시함 
      *	hover에 회사명/세부직무명/클러스터 라벨을 노출하여 탐색 가능하도록 구성함
        
    5-4. 6대 핵심역량 점수 반영 시각화(레이더/평균/Top-N)
      *	공고별 6대 핵심역량(Python/R, SQL/DB, 시각화, ML/AI, 도메인지식, 협업)을 LLM 프롬프트로 점수화하고 JSON으로만 출력하도록 강제함 
      *	산점도에서 선택된 공고에 대해 6역량 점수를 Radar Chart로 시각화함 
      *	클러스터 평균/세부직무 평균을 6축 평균 점수로 계산하여 레이더 및 표로 제공함 
      *	선택된 클러스터/세부직무 내부에서 6역량 합 상위 N개를 Top-N으로 선정하고, Top-N 평균 레이더 및 Top-N 리스트를 제공함 

## 💡 Usage
*	Streamlit 환경에서의 원활한 실행과 응답 속도 확보를 위해, LLM이 필요한 연산 과정은 사전에 로컬 환경에서 실행한 후 결과 파일을 불러오는 구조로 설계함
*	final_df_2는 채용공고 크롤링·OCR·전처리 및 세부직무 분류까지 완료된 최종 데이터 파일임
*	llm_6_skills_cache.json, meta_info.json은 LLM 기반 6대 핵심역량 산출 결과를 저장한 파일임
*	precomputed_job_map.parquet은 임베딩·클러스터링·UMAP을 통해 계산된 직무 유사도 결과 파일임
*	위 파일들을 지정된 경로에 저장한 후, 해당 디렉터리에서 아래 명령어를 실행하면 Streamlit 대시보드를 확인할 수 있음
*	streamlit run app.py

## 🚀 Tech Stack
🔤 자연어 처리 (NLP)

    •	Sentence-BERT (SBERT)
      : 채용공고 텍스트 임베딩 생성, 담당업무·우대사항·스킬 기반 직무 유사도 계산
      
    •	정규식 기반 Rule-based NLP
      : 광고·푸터·불필요 문장 제거, 섹션 분리(담당업무, 우대사항 등) 및 키워드 탐지
  
🤖 Large Language Model (LLM)

    •	LLM (Transformers 기반 Local / API 모델)
      : 세부직무명 분류, 6대 핵심역량(Python/R, SQL/DB, 시각화, ML/AI, 도메인지식, 협업) 점수 산출
      
    •	Prompt Engineering
      : JSON 출력 강제, 새로운 라벨 생성 금지, 입력 텍스트 길이 제한 및 생성 안정화
    
📊 추천·분석 알고리즘

    •	KMeans Clustering
      : 임베딩 기반 세부직무 군집화
      
    •	UMAP
      : 직무 간 유사도 시각화를 위한 차원 축소
      
    •	Rule-based Scoring & Ranking
      : 클러스터 특성 분석, 6대 핵심역량 합 기반 Top-N 채용공고 추천
  
📈 데이터 시각화 & 대시보드

    •	Streamlit
      : 전체 분석 결과 인터랙티브 대시보드 구현
      
    •	Plotly / Plotly Express
      : UMAP 산점도(직무 유사도), Radar Chart(직무 평균 vs 개인 역량 비교)
      
    •	Pandas / NumPy
      : 데이터 처리 및 집계  

🧱 데이터 수집·전처리

    •	Playwright
      : 사람인 채용공고 PDF 크롤링  
      
    •	PaddleOCR
      : PDF 기반 채용공고 텍스트 OCR
      
    •	Python (3.11.14)
      : 전체 파이프라인 구현

## 🤝 Contributing
…

## 📄 License
